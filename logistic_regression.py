# -*- coding: utf-8 -*-
"""LOGISTIC_REGRESSION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yI7s_elLw2o61zOaa3eLIrMHqepWK7Kj

# **MODULES IMPORT**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math

"""# **FEATURE PROCESSING**"""

data1 = pd.read_csv('data.csv')

data1.head(2)

x1 = np.array(data1['radius_mean'].values)
x2 = np.array(data1['smoothness_mean'].values)
x = [x1,x2]

degree = 5
iteration = 100000

y = data1['diagnosis'].values
for i in range (0,len(y)):
  if(y[i]=='M'):
    y[i]=1
  else:
    y[i]=0

"""# **FORMULAS**

**Sigmoid Function**
$$f_{(w,b)}(z) = \frac{1}{1+e^{-z}}$$
$$z = w.x+b$$
*It returns values between 0 and 1*
<br><br>
**COST FUNCTION**
$$j_{(w,b)} = \frac{1}{m}\sum_{i=1}^{m}[loss(f_{(w,b)}(x^i),y^i]$$
$$loss = -y^i.log(f_{(w,b)}x^i)-(1-y^i).log(1-f_{(w,b)}x^i)$$
*Here the loss fuction helps to penalize the wrong output more*
<br><br>
**GRADIENT FUNCTION**

$$\frac{dJ_{(w,b)}}{dw_j} = \frac{1}{m}\sum_{i=1}^{m}(f_{(w,b)}(x^i)-y^i).x_j^i$$
$$\frac{dJ_{(w,b)}}{db} = \frac{1}{m}\sum_{i=1}^{m}(f_{(w,b)}(x^i)-y^i).1$$

*Here this function is similar to that of linear regression but the output function is different*
<br><br>
**GRADIENT DESCENT FUNCTION**

$$w_j = w_j - \alpha.\frac{dJ_{(w,b)}}{dw_j}$$
$$b = b - \alpha.\frac{dJ_{(w,b)}}{db}$$
*It helps to reach parametrical values efficiently*

# **ALGORITHM**
"""

def classification2(x, y,degree,itr):
    # standardisation
    def standard(arr):
        return (arr - np.mean(arr)) / np.std(arr)

    # Polynomial Features Formation
    def polynomialFeature(x, degree):
        def power(arr, p):
            return arr ** p

        f = []
        for i in range(len(x)):
            f.append(x[i])
        for i in range(2,degree+1):
            for j in range(len(x)):
                powered_feature = power(x[j], i)
                f.append(powered_feature)
        return f

    # Formation of multi-dimensional Feature array
    def twod(f):
        arr = np.zeros((len(f[0]), len(f)))
        for i in range(0, len(f[0])):
            for j in range(0, len(f)):
                arr[i][j] = f[j][i]
        return arr

    # Splitting into train and test data
    def ttsplit(x):
        m = int(0.75 * x.shape[0])
        train = x[:m, :]
        test = x[m:, :]
        return train, test

    # logistic regression algorithm
    def logisticRegression(x, y, alpha, iteration):
        # Sigmoid Function
        def sigmoid(z):
            return 1 / (1 + np.exp(-z))

        # Cost Function
        def cost(x, y, w, b):
            m, n = x.shape
            s = 0
            for i in range(m):
                f_wb = sigmoid(np.dot(w, x[i]) + b)
                f_wb = np.clip(f_wb, 1e-10, 1 - 1e-10)  # Clipping to avoid log(0)
                s += -y[i] * np.log(f_wb) - (1 - y[i]) * np.log(1 - f_wb)
            return s / m

        # Gradient Function
        def gradient(x, y, w, b):
            m, n = x.shape
            dj_dw = np.zeros(n)
            dj_db = 0
            for i in range(0, m):
                x[i] = np.array(x[i])
                error = sigmoid(np.dot(w, x[i]) + b) - y[i]
                for j in range(0, n):
                    dj_dw[j] += error * x[i][j]
                dj_db += error
            return dj_dw / m, dj_db / m

        # Gradient Descent Function
        def descent(x, y, w, b, alpha, iteration):
            j_hist = []
            for i in range(0, iteration):
                dj_dw, dj_db = gradient(x, y, w, b)
                for j in range(0, len(dj_dw)):
                    w = w- alpha * dj_dw
                b = b - alpha * dj_db
                j_hist.append(cost(x, y, w, b))
                if int(i % int(0.1*iteration)) == 0:
                    print(f'iteration[{i}] - cost: {cost(x, y, w, b)}')
            print(f'iteration[{iteration-1}] - cost: {cost(x, y, w, b)}')
            return w, b , j_hist

        # Input
        w, b, jhist = descent(x, y, np.zeros(x.shape[1]), 0, alpha, iteration)
        return w, b, jhist

    # Level 1 - standardisation
    for i in range(0, len(x)):
        x[i] = standard(x[i])

    # Level 2 - Polynomial feature formation
    f = polynomialFeature(x, degree)

    # Level 3 - Formation of multi-dimensional Feature array
    arr = twod(f)

    # Level 4 - split into train and test
    x_train, x_test = ttsplit(arr)
    y_train = y[:int(0.75 * len(y))]
    y_test = y[int(0.75 * len(y)):]

    # Level 5 - logistic regression
    alpha = 0.01
    iteration = itr
    w, b, jhist = logisticRegression(x_train, y_train, alpha, iteration)

    return w, b, x_train, x_test, y_train, y_test, jhist

w,b,x_train,x_test,y_train,y_test,jhist = classification2(x,y,degree,iteration)

"""# **PLOTTING**"""



def dscboundary(x, y, w, b,degree,ch):
    def sigmoid(z):
        return np.exp(-np.maximum(z, 0)) / (1 + np.exp(-np.abs(z)))

    def polynomialFeature(x, degree):
        def power(arr, p):
            return arr ** p

        f = []
        for i in range(x.shape[1]):
            f.append(x[:, i])
        for i in range(2, degree + 1):
            for j in range(x.shape[1]):
                powered_feature = power(x[:, j], i)
                f.append(powered_feature)
        return np.column_stack(f)

    # Generating mesh grid for plotting
    x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1
    y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1

    xmesh, ymesh = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))

    # Creating grid of points to evaluate
    grid = np.c_[xmesh.ravel(), ymesh.ravel()]
    gridpoly = polynomialFeature(np.column_stack((grid, np.zeros((len(grid), 0)))), degree)

    # Computing sigmoid on the linear combination of the features
    zmesh = sigmoid(np.dot(gridpoly, w) + b)
    zmesh = zmesh.reshape(xmesh.shape)

    # Plotting the contour and the training points

    fig, ax = plt.subplots()

    #facecolor,title,axes,labels
    theme_dark = ['#121212','#0abb92','#7F7B7B','#ffffff','#1D1C1C']
    theme_atmos = ['#1b1e2b','#31CBC3','#80cbc4','#CCCFDA','#292d3e']
    theme_default = ['white','#000000','#000000','#000000','white']

    if(ch == 0):
      theme = theme_dark
    elif(ch==1):
      theme = theme_atmos
    else:
      theme = theme_default

    fig.patch.set_facecolor(theme[0])
    ax.set_facecolor(theme[4])


    # ax.contourf(xmesh, ymesh, zmesh, levels=[0, 0.5, 1], colors=['#e98585', '#bfc8e0'], alpha=1)
    ax.contour(xmesh, ymesh, zmesh, levels=[0.5], colors='yellow')

    ax.scatter(x[y == 0][:, 0], x[y == 0][:, 1], c='#479b1a', marker='s', label='Benign')
    ax.scatter(x[y == 1][:, 0], x[y == 1][:, 1], c='#cbd1c5', marker='v', label='Malignant')

    ax.set_xlabel('radius')
    ax.set_ylabel('smoothness')

    ax.set_title('Logistic Regression Model for Breast Cancer',color=theme[1])

    ax.tick_params(axis='x', colors=theme[2])
    ax.tick_params(axis='y', colors=theme[2])

    ax.xaxis.label.set_color(theme[3])
    ax.yaxis.label.set_color(theme[3])

    legend = ax.legend()
    legend.get_frame().set_facecolor('#292d3e')

    for text in legend.get_texts():
        text.set_color('white')

    plt.show()

dscboundary(x_train , y_train, w, b,degree,1)

fig, ax = plt.subplots()

theme= ['#1b1e2b','#31CBC3','#80cbc4','#CCCFDA','#292d3e']

fig.patch.set_facecolor(theme[0])
ax.set_facecolor(theme[4])

ax.set_xlabel('iterations')
ax.set_ylabel('cost')

ax.set_title(f'Decrease of Error(cost) on {iteration} iterations',color=theme[1])

ax.tick_params(axis='x', colors=theme[2])
ax.tick_params(axis='y', colors=theme[2])

ax.xaxis.label.set_color(theme[3])
ax.yaxis.label.set_color(theme[3])

xaxis = np.arange(0,iteration,1)
ax.plot(xaxis,jhist)

"""$$f_{(w,b)}(z) = \frac{1}{1+e^{-z}}$$
$$z = w.x+b$$
<br><br>
$$j_{(w,b)} = \frac{1}{m}\sum_{i=1}^{m}[loss(f_{(w,b)}(x^i),y^i]$$
$$loss = -y^i.log(f_{(w,b)}x^i)-(1-y^i).log(1-f_{(w,b)}x^i)$$
<br><br>
$$\frac{dJ_{(w,b)}}{dw_j} = \frac{1}{m}\sum_{i=1}^{m}(f_{(w,b)}(x^i)-y^i).x_j^i$$
$$\frac{dJ_{(w,b)}}{db} = \frac{1}{m}\sum_{i=1}^{m}(f_{(w,b)}(x^i)-y^i).1$$
<br><br>
$$w_j = w_j - \alpha.\frac{dJ_{(w,b)}}{dw_j}$$
$$b = b - \alpha.\frac{dJ_{(w,b)}}{db}$$
"""

